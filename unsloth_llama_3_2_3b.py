# -*- coding: utf-8 -*-
"""unsloth/Llama-3.2-3B.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUcDSpuBYg3gJ1u1nJzvr8ZCF68rXq9U

# Data prep
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

"""Датасет сокращался три раза, код удалять не стала, но закомментировала"""

#df = pd.read_csv('/content/drive/MyDrive/Project_final/arxiv_parsed_data.csv')

#df

#df_full_texts = df.iloc[:, :-1].copy()
#df_full_texts = df_full_texts[~df_full_texts['full_text'].str.startswith('N/A', na=False)]
#df_full_texts = df_full_texts.dropna(subset=['full_text'])

#df_full_texts.head()

#len(df_full_texts)

#from google.colab import files

##filename = 'arxiv_data_cleaned.csv'
#df_full_texts.to_csv(filename, index=False, encoding='utf-8')

#files.download(filename)

"""Сокращенный вариант датасета (в итоге пришлось взять только 20% от первоначального)"""

#df_clean = df[~df['full_text'].str.startswith('N/A', na=False)]

#df_half = df_clean.sample(frac=0.5, random_state=42)

#filename_2 = 'arxiv_half.csv'
#df_half.to_csv('arxiv_half.csv', index=False)

#files.download(filename_2)

#сохранение на диск
#output_path = '/content/drive/MyDrive/Project_final/arxiv_half.csv'
#df_half.to_csv(output_path, index=False, encoding='utf-8')

df_full_texts = pd.read_csv('/content/drive/MyDrive/Project_final/arxiv_0.2.csv')

#эту часть не запускаю, так как соранила датасет уже с удаленным пустыми строками и очищенными текстами
#df_full_texts = df_full_texts.dropna(subset=['full_text'])
#df_full_texts = df_full_texts[df_full_texts['full_text'].apply(lambda x: isinstance(x, str))]

len(df_full_texts)

df_full_texts.head()

# @title Preprocessing
#import nltk
#from nltk.corpus import stopwords
#stop_words = stopwords.words('english')

"""Не стала предобрабатывать тексты, поэтому эта часть также закомментирована"""

#!python -m spacy download en_core_web_sm

#import spacy
#nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

#def preprocess(text):
  #doc = nlp(text.lower())
  #lemmas = [word.lemma_ for word in doc if word.lemma_ not in stop_words and word.lemma_[0].isalpha()]
  #return " ".join(lemmas)

#df_full_texts['preprocessed_text'] = df_full_texts['full_text'].apply(preprocess)

"""# Unsloth

Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# if "COLAB_" not in "".join(os.environ.keys()):
#     !pip install unsloth
# else:
#     !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo
#     !pip install sentencepiece protobuf "datasets>=3.4.1" huggingface_hub hf_transfer
#     !pip install --no-deps unsloth

from unsloth import FastModel
import torch
from trl import SFTTrainer, SFTConfig
from transformers import TrainingArguments

model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/Llama-3.2-3B",
    max_seq_length = 1024,
    load_in_4bit = True,
    load_in_8bit = False,
    full_finetuning = False
)

# @title LoRa adapters
model = FastModel.get_peft_model(
    model,
    finetune_vision_layers     = False,
    finetune_language_layers   = True,
    finetune_attention_modules = True,
    finetune_mlp_modules       = True,

    r = 8,
    target_modules = ["q_proj", "v_proj"],
    lora_alpha = 8,
    lora_dropout = 0,
    bias = "none",
    random_state = 3407,
)

"""AdaLoRA"""

#from peft import AdaLoraConfig
#adalora_config = AdaLoraConfig(
        #init_r=12,      # начальный ранк
        #target_r=8,     # целевой ранк
        #beta1=0.85,
        #beta2=0.85,
        #target_modules=["q_proj", "v_proj"]
    #)

# @title More data prep
from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3",
)

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict

def formatting_prompts_func(examples):
    inputs = examples["full_text"]
    outputs = examples["subjects"]
    texts = []

    for input_text, output_text in zip(inputs, outputs):
        truncated_input = str(input_text)[:5000]

        messages = [
            {
                "role": "system",
                "content": "You are a professional academic assistant. Your task is to classify research paper excerpts into their respective arXiv subjects."
            },
            {
                "role": "user",
                "content": f"Analyze this paper excerpt and provide its subject categories:\n\n{truncated_input}."
            },
            {
                "role": "assistant",
                "content": output_text
            },
        ]

        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        texts.append(text)

    return { "text" : texts }

dataset = Dataset.from_pandas(df_full_texts[['full_text', 'subjects']])
dataset = dataset.map(formatting_prompts_func, batched = True)

print("Пример готового промпта для обучения:")
print(dataset[0]["text"])

dataset = dataset.train_test_split(test_size = 0.1)

train_dataset = dataset["train"]
eval_dataset = dataset["test"]

# @title Training
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_dataset,
    eval_dataset = eval_dataset,
    max_seq_length = 1024,
    args = SFTConfig(
        dataset_text_field = "text",
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 8,
        warmup_steps = 5,
        num_train_epochs = 1,
        #max_steps = None,
        learning_rate = 2e-5,
        logging_steps = 1,

        output_dir = "outputs",
        eval_strategy = "no",
        eval_steps = 50,

        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        report_to = "none",
    ),
)

from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|start_header_id|>user<|end_header_id|>\n\n",
    response_part = "<|start_header_id|>assistant<|end_header_id|>\n\n",
)

tokenizer.decode(trainer.train_dataset[100]["input_ids"])

tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100]["labels"]])

# @title Show current memory stats
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

trainer_stats = trainer.train(resume_from_checkpoint = False)

# @title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)

if 'train_runtime' in trainer_stats.metrics:
    train_runtime = trainer_stats.metrics['train_runtime']
    print(f"{train_runtime} seconds used for training.")
    print(f"{round(train_runtime/60, 2)} minutes used for training.")
else:
    print("Train runtime metric not found.")

print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

# @title Inference
# TextStreamer

from unsloth.chat_templates import get_chat_template
tokenizer = get_chat_template(
    tokenizer,
    chat_template = "llama-3",
)

messages = [
    {"role": "system", "content": "You are a professional academic assistant. Your task is to classify research paper excerpts into their respective arXiv subjects."},
    {"role": "user", "content": f"Analyze this paper excerpt and provide its subject categories:\n\n{truncated_input}."}
]
text = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    tokenize=False,
)

from transformers import TextStreamer
_ = model.generate(
    **tokenizer(text=[text], return_tensors = "pt").to("cuda"),
    max_new_tokens = 128,
    temperature = 0.1, top_p = 0.9, top_k = 64,
    do_sample=True,
    streamer = TextStreamer(tokenizer, skip_prompt = True),
)

messages = [
    {"role": "system", "content": "You are a professional academic assistant. Your task is to classify research paper excerpts into their respective arXiv subjects."},
    {"role": "user", "content": f"Analyze this paper excerpt and provide its subject categories:\n\n{truncated_input}"}
]
text = tokenizer.apply_chat_template(
    messages,
    tokenize=False,
    add_generation_prompt=True
)

inputs = tokenizer(text, return_tensors="pt").to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    temperature=0.1,
    do_sample=True
)

new_tokens = outputs[0][inputs.input_ids.shape[-1]:]
clean_answer = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()

print(f"Предсказанная категория: {clean_answer}")

# @title Saving LoRa adapters
model.save_pretrained("lora_model")
tokenizer.save_pretrained("lora_model")

# @title Saving model
if True:
    model.save_pretrained_merged("llama-3-finetuned-merged", tokenizer, save_method = "merged_16bit")

if False: # Поменять на True для формата GGUF
    model.save_pretrained_gguf(
        "llama-3-finetuned-gguf",
        tokenizer,
        quantization_method = "q8_0",
    )
