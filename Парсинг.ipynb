{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задача – собрать ссылки по годам и месяцам, с каждой ссылки собрать авторов, названия статей, ссылки на html файлы, темы, затем с каждой ссылки на html файл собрать текст стаьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерируем ссылки\n",
    "import itertools\n",
    "\n",
    "def generate_urls(category='cs', start_year=2023, end_year=2026, articles_per_page=2000, max_pages_per_month=5):\n",
    "    base_url = \"https://arxiv.org/list/\"\n",
    "    urls = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            month_str = f\"{month:02d}\"\n",
    "            date_segment = f\"{year}-{month_str}\"\n",
    "\n",
    "            for page_num in range(max_pages_per_month):\n",
    "                skip = page_num * articles_per_page\n",
    "                url = f\"{base_url}{category}/{date_segment}?skip={skip}&show={articles_per_page}\"\n",
    "                urls.append(url)\n",
    "    return urls\n",
    "\n",
    "generated_urls = generate_urls(\n",
    "    category='cs', \n",
    "    start_year=2023, \n",
    "    end_year=2026, \n",
    "    articles_per_page=2000,\n",
    "    max_pages_per_month=5\n",
    ")\n",
    "\n",
    "print(\"Первые 10 URL:\")\n",
    "for i, url in enumerate(generated_urls[:10]):\n",
    "    print(url)\n",
    "\n",
    "print(\"\\nПоследние 10 URL:\")\n",
    "for i, url in enumerate(generated_urls[-10:]):\n",
    "    print(url)\n",
    "\n",
    "print(f\"\\nВсего сгенерировано URL: {len(generated_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция для извлечения текста статей\n",
    "\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "def extract_full_text(html_url):\n",
    "    if not html_url:\n",
    "        return \"N/A\"\n",
    "\n",
    "    try:\n",
    "        time.sleep(1) \n",
    "        \n",
    "        response = requests.get(html_url, headers=HEADERS, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup_html = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        content_div = soup_html.find('div', {'class':\"ltx_page_content\"})\n",
    "        \n",
    "        if content_div:\n",
    "            full_text = content_div.get_text(separator=' ', strip=True)\n",
    "            return full_text\n",
    "        else:\n",
    "            return \"N/A - Content div not found\"\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"N/A - Error loading HTML page: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"N/A - General error extracting HTML text: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для извлечения всей необходимой информации по статьям с каждой страницы\n",
    "\n",
    "def extract_all_article_details(url):\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при загрузке списка: {e}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    articles_list = soup.find('dl', id='articles')\n",
    "    if not articles_list:\n",
    "        return []\n",
    "\n",
    "    dts = articles_list.find_all('dt')\n",
    "    dds = articles_list.find_all('dd')\n",
    "    \n",
    "    all_articles_data = []\n",
    "\n",
    "    for dt, dd in zip(dts, dds):\n",
    "\n",
    "        html_link_tag = dt.find('a', title='View HTML')\n",
    "        \n",
    "        html_url = None\n",
    "        full_text = \"N/A - No HTML version\" \n",
    "\n",
    "        if html_link_tag:\n",
    "            html_url = urljoin(\"https://arxiv.org\", html_link_tag['href'])\n",
    "            \n",
    "            print(f\"Парсим текст для статьи: {html_url}\")\n",
    "            full_text = extract_full_text(html_url)\n",
    "        else:\n",
    "            print(\"HTML версия отсутствует, пропускаем...\")\n",
    "\n",
    "        t_div = dd.find('div', class_='list-title')\n",
    "        title = t_div.get_text(strip=True).replace('Title:', '') if t_div else \"N/A\"\n",
    "        a_div = dd.find('div', class_='list-authors')\n",
    "        authors = a_div.get_text(strip=True).replace('Authors:', '') if a_div else \"N/A\"\n",
    "        s_div = dd.find('div', class_='list-subjects')\n",
    "        subjects = s_div.get_text(strip=True).replace('Subjects:', '') if s_div else \"N/A\"\n",
    "\n",
    "        all_articles_data.append({\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'subjects': subjects,\n",
    "            'html_url': html_url,\n",
    "            'full_text': full_text\n",
    "        })\n",
    "    \n",
    "    return all_articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#собираем информацию со всех сгенирированных ссылок и создаем data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'arxiv_parsed_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_parser(generated_urls):\n",
    "    all_data = []\n",
    "    \n",
    "    # Загружаем уже существующие данные, чтобы продолжить с места остановки\n",
    "    processed_urls = set()\n",
    "    if os.path.exists(FILE):\n",
    "        existing_df = pd.read_csv(FILE)\n",
    "        if 'source_page_url' in existing_df.columns:\n",
    "            processed_urls = set(existing_df['source_page_url'].unique())\n",
    "            all_data = existing_df.to_dict('records')\n",
    "            print(f\"Найдено сохраненных данных. Пропущено страниц: {len(processed_urls)}\")\n",
    "\n",
    "    # Основной цикл по списку сгенерированных ссылок\n",
    "    for url in generated_urls:\n",
    "        if url in processed_urls:\n",
    "            continue\n",
    "            \n",
    "        print(f\"Начинаю обработку страницы: {url}\")\n",
    "        \n",
    "        try:\n",
    "            articles = extract_all_article_details(url)\n",
    "            \n",
    "            if articles:\n",
    "                for a in articles:\n",
    "                    a['source_page_url'] = url\n",
    "                \n",
    "                all_data.extend(articles)\n",
    "                \n",
    "                df_temp = pd.DataFrame(all_data)\n",
    "                df_temp.to_csv(FILE, index=False, encoding='utf-8')\n",
    "                \n",
    "                print(f\"Успешно: собрано {len(articles)} статей. Данные обновлены в {FILE}\")\n",
    "            \n",
    "            processed_urls.add(url)\n",
    "            \n",
    "            time.sleep(2)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"КРИТИЧЕСКАЯ ОШИБКА на странице {url}: {e}\")\n",
    "            print(\"Все собранные данные сохранены. Можно запустить код снова позже.\")\n",
    "            if all_data:\n",
    "                pd.DataFrame(all_data).to_csv(FILE, index=False, encoding='utf-8')\n",
    "            continue \n",
    "\n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_df = main_parser(generated_urls)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
